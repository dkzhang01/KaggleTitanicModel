# KaggleTitanicModel

Created to competite in the Titanic - Machine Learning from Disaster Competition on Kaggle
All the code used is contained as a jupyter notebook (.ipynb) in the repository.

It starts by preparing the data for the models to use. Then it uses a few different popular models: Support Vector Machine, Random Forest Classifier, Decision Tree Classifier, K-Nearest Neighbors, Naive Bayes, Neural Networks. I tried using an ensemble method utilizing all of the models above. However, just the Support Vector Machine ended with the highest accuracy of 0.77511
